{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from train import train\n",
    "import priors\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_openml_list, valid_dids_classification, test_dids_classification\n",
    "from tabular import evaluate, get_model, get_default_spec\n",
    "from tabular import bayes_net_metric, gp_metric, knn_metric, catboost_metric, xgb_metric, logistic_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test datasets...\n",
      "wine 973\n",
      "covertype 1596\n",
      "\n",
      " Loading valid datasets...\n",
      "ionosphere 59\n"
     ]
    }
   ],
   "source": [
    "### Loads small list of datasets\n",
    "print('Loading test datasets...')\n",
    "test_datasets, test_datasets_df = load_openml_list(test_dids_classification[0:2], filter_for_nan=True)\n",
    "ds = test_datasets\n",
    "print('\\n Loading valid datasets...')\n",
    "valid_datasets, valid_datasets_df = load_openml_list(valid_dids_classification[0:2], filter_for_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loads all datasets\n",
    "print('Loading test datasets...')\n",
    "test_datasets, test_datasets_df = load_openml_list(test_dids_classification, filter_for_nan=True)\n",
    "ds = test_datasets\n",
    "\n",
    "print('\\n Loading valid datasets...')\n",
    "valid_datasets, valid_datasets_df = load_openml_list(valid_dids_classification, filter_for_nan=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After how many training samples should evaluatuion be done?\n",
    "# Trained models have not been trained to evaluate after 30 samples\n",
    "# so performance will drop\n",
    "eval_positions = [30]\n",
    "\n",
    "# What is the maximum number of features?\n",
    "# Pretrained models have to use 60\n",
    "max_features = 60\n",
    "\n",
    "# How many samples should be loaded for one dataset?\n",
    "# Samples after the training sequence are used for evaluation\n",
    "seq_len = 100\n",
    "\n",
    "# How many subsamples of datasets should be drawn for each dataset\n",
    "max_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model_checkpoint_dir = \"../results/tabular_model_gp.ckpt\"\n",
    "gp_model_config = {'batch_size': 512,\n",
    " 'bptt': 100,\n",
    " 'dropout': 0.5,\n",
    " 'emsize': 512,\n",
    " 'epochs': 100,\n",
    " 'eval_positions': [10, 20, 40, 80],\n",
    " 'lr': 6.271726842985807e-05,\n",
    " 'nhead': 4,\n",
    " 'nhid_factor': 2,\n",
    " 'nlayers': 5,\n",
    " 'num_features': 60,\n",
    " 'prior_lengthscale': 0.00014803074521613278,\n",
    " 'prior_noise': 0.001,\n",
    " 'prior_normalize_by_used_features': True,\n",
    " 'prior_num_features_used_sampler': {'uniform_int_sampler_f(1,max_features)': '<function <lambda>.<locals>.<lambda> at 0x7f21e832e550>'},\n",
    " 'prior_order_y': False,\n",
    " 'prior_outputscale': 2.3163584733185836,\n",
    " 'prior_type': 'gp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_model_checkpoint_dir = \"../results/tabular_model_bnn.ckpt\"\n",
    "bnn_model_config = {'batch_size': 512,\n",
    " 'bptt': 50,\n",
    " 'dropout': 0.5,\n",
    " 'emsize': 512,\n",
    " 'epochs': 100,\n",
    " 'eval_positions': [10, 20, 40],\n",
    " 'lr': 1.6421403128751275e-05,\n",
    " 'nhead': 4,\n",
    " 'nhid_factor': 2,\n",
    " 'nlayers': 5,\n",
    " 'num_features': 60,\n",
    " 'prior_activations': \"<class 'torch.nn.modules.activation.Tanh'>\",\n",
    " 'prior_dropout_sampler': {'lambda: 0.0': '<function <lambda> at 0x7f613c1364c0>'},\n",
    " 'prior_emsize_sampler': {'scaled_beta_sampler_f(2.0, 4.0, 150, 2)': '<function <lambda>.<locals>.<lambda> at 0x7f613c136310>'},\n",
    " 'prior_is_causal': False,\n",
    " 'prior_nlayers_sampler': {'lambda: 3': '<function <lambda> at 0x7f613c136790>'},\n",
    " 'prior_noise_std_gamma_k': 1.8663049257557085,\n",
    " 'prior_noise_std_gamma_theta': 0.05275478076173361,\n",
    " 'prior_normalize_by_used_features': False,\n",
    " 'prior_num_features_used_sampler': {'scaled_beta_sampler_f(1.0, 1.6, max_features, 2)': '<function <lambda>.<locals>.<lambda> at 0x7f613c136550>'},\n",
    " 'prior_order_y': True,\n",
    " 'prior_sigma_gamma_k': 3.6187797729244253,\n",
    " 'prior_sigma_gamma_theta': 0.06773738681062867,\n",
    " 'prior_type': 'mlp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu:0 device\n",
      "DataLoader.__dict__ {'num_steps': 100, 'fuse_x_y': False, 'get_batch_kwargs': {'batch_size': 512, 'seq_len': 50, 'num_features': 60, 'hyperparameters': ('<function <lambda> at 0x7f613c136790>', '<function <lambda>.<locals>.<lambda> at 0x7f613c136310>', \"<class 'torch.nn.modules.activation.Tanh'>\", <function <lambda>.<locals>.<lambda> at 0x7f2ed182ba60>, <function <lambda>.<locals>.<lambda> at 0x7f2ff03b7040>, '<function <lambda> at 0x7f613c1364c0>', True, '<function <lambda>.<locals>.<lambda> at 0x7f613c136550>', None, False, None, None, None, True, False, None, 0.0), 'batch_size_per_gp_sample': 8}, 'num_features': 60, 'num_outputs': 1}\n"
     ]
    }
   ],
   "source": [
    "model_type = 'bnn'\n",
    "if model_type == 'gp':\n",
    "    raise Exception(\"Not Implemented\")\n",
    "    config = gp_model_config\n",
    "    checkpoint_dir = gp_model_checkpoint_dir\n",
    "elif model_type == 'bnn':\n",
    "    config = bnn_model_config\n",
    "    checkpoint_dir = bnn_model_checkpoint_dir\n",
    "\n",
    "model = get_model(config, device, eval_positions, should_train=False) # simply get Transformer model\n",
    "model_state, _ = torch.load(checkpoint_dir) \n",
    "model[2].load_state_dict(model_state) # load state dict for the model\n",
    "model = model[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of PFN and Baselines on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n",
      "\t Eval position 30 done..\n",
      "Evaluating covertype\n",
      "\t Eval position 30 done..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'auc',\n",
       " 'wine_mean_metric_at_30': 0.9587367346938775,\n",
       " 'wine_time': 1.9455327987670898,\n",
       " 'covertype_mean_metric_at_30': 0.9624857142857144,\n",
       " 'covertype_time': 2.1963138580322266,\n",
       " 'mean_metric_at_30': 0.960611224489796,\n",
       " 'mean_metric': 0.960611224489796}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "result = evaluate(ds, model.to(device), 'transformer'\n",
    "                  , max_features = max_features\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , rescale_features=config[\"prior_normalize_by_used_features\"]\n",
    "                  , extend_features=True, plot=False, overwrite=True, save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/20 [00:00<?, ?it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▏                                                                                           | 2/20 [00:00<00:01, 14.78it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████▍                                                                                 | 4/20 [00:00<00:01, 15.12it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████▋                                                                  | 7/20 [00:00<00:00, 19.44it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      " 50%|██████████████████████████████████████████████████▌                                                  | 10/20 [00:00<00:00, 22.09it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      " 65%|█████████████████████████████████████████████████████████████████▋                                   | 13/20 [00:00<00:00, 23.99it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████▊                    | 16/20 [00:00<00:00, 24.88it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████▉     | 19/20 [00:00<00:00, 25.45it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 23.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n",
      "Evaluating covertype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/20 [00:00<?, ?it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████████▎                                                                                      | 3/20 [00:00<00:00, 28.16it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      " 30%|██████████████████████████████▌                                                                       | 6/20 [00:00<00:00, 27.25it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      " 45%|█████████████████████████████████████████████▉                                                        | 9/20 [00:00<00:00, 27.52it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      " 60%|████████████████████████████████████████████████████████████▌                                        | 12/20 [00:00<00:00, 27.21it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████▊                         | 15/20 [00:00<00:00, 27.41it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████▉          | 18/20 [00:00<00:00, 27.40it/s]/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:211: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "\r\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 27.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'auc',\n",
       " 'wine_mean_metric_at_30': 0.9002846938775511,\n",
       " 'wine_time': 0.8868608474731445,\n",
       " 'covertype_mean_metric_at_30': 0.7934336734693876,\n",
       " 'covertype_time': 0.74686598777771,\n",
       " 'mean_metric_at_30': 0.8468591836734694,\n",
       " 'mean_metric': 0.8468591836734694}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(ds, knn_metric, 'knn'\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , overwrite=True\n",
    "                  , save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:19<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n",
      "Evaluating covertype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:34<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'auc',\n",
       " 'wine_mean_metric_at_30': 0.9295571428571429,\n",
       " 'wine_time': 19.71760869026184,\n",
       " 'covertype_mean_metric_at_30': 0.9083142857142859,\n",
       " 'covertype_time': 34.93118238449097,\n",
       " 'mean_metric_at_30': 0.9189357142857144,\n",
       " 'mean_metric': 0.9189357142857144}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(ds, logistic_metric, 'logistic'\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , overwrite=True\n",
    "                  , save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:48<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n",
      "Evaluating covertype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:58<00:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'auc',\n",
       " 'wine_mean_metric_at_30': 0.7852867346938774,\n",
       " 'wine_time': 48.288535356521606,\n",
       " 'covertype_mean_metric_at_30': 0.9294663265306123,\n",
       " 'covertype_time': 58.08171844482422,\n",
       " 'mean_metric_at_30': 0.857376530612245,\n",
       " 'mean_metric': 0.857376530612245}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(ds, gp_metric, 'gp'\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , overwrite=True\n",
    "                  , save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                                              | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/home/ypq/anaconda3/envs/GD/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "  5%|█████                                                                                                 | 1/20 [01:05<20:45, 65.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   8.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10%|██████████▏                                                                                           | 2/20 [02:04<18:30, 61.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   9.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 15%|███████████████▎                                                                                      | 3/20 [02:56<16:14, 57.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   9.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|████████████████████▍                                                                                 | 4/20 [03:58<15:44, 59.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=  10.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 25%|█████████████████████████▌                                                                            | 5/20 [04:59<14:54, 59.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|██████████████████████████████▌                                                                       | 6/20 [05:56<13:46, 59.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 35%|███████████████████████████████████▋                                                                  | 7/20 [06:54<12:40, 58.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   7.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   6.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████████████████████████████████████████▊                                                             | 8/20 [07:53<11:43, 58.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 45%|█████████████████████████████████████████████▉                                                        | 9/20 [08:45<10:24, 56.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|██████████████████████████████████████████████████▌                                                  | 10/20 [09:45<09:36, 57.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 55%|███████████████████████████████████████████████████████▌                                             | 11/20 [10:41<08:35, 57.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|████████████████████████████████████████████████████████████▌                                        | 12/20 [11:44<07:52, 59.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 65%|█████████████████████████████████████████████████████████████████▋                                   | 13/20 [12:41<06:47, 58.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70%|██████████████████████████████████████████████████████████████████████▋                              | 14/20 [13:43<05:57, 59.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 75%|███████████████████████████████████████████████████████████████████████████▊                         | 15/20 [14:43<04:57, 59.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████▊                    | 16/20 [15:41<03:56, 59.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 85%|█████████████████████████████████████████████████████████████████████████████████████▊               | 17/20 [16:40<02:57, 59.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████▉          | 18/20 [17:40<01:58, 59.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   6.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████▉     | 19/20 [18:44<01:00, 60.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   7.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   6.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [19:44<00:00, 59.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "\t Eval position 30 done..\n",
      "Evaluating covertype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                                              | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  5%|█████                                                                                                 | 1/20 [00:52<16:37, 52.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10%|██████████▏                                                                                           | 2/20 [01:52<17:09, 57.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 1.0, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   8.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 15%|███████████████▎                                                                                      | 3/20 [02:42<15:09, 53.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 1.0, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|████████████████████▍                                                                                 | 4/20 [03:47<15:31, 58.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 25%|█████████████████████████▌                                                                            | 5/20 [04:56<15:28, 61.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 2, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|██████████████████████████████▌                                                                       | 6/20 [05:49<13:44, 58.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 35%|███████████████████████████████████▋                                                                  | 7/20 [06:53<13:10, 60.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████████████████████████████████████████▊                                                             | 8/20 [08:02<12:38, 63.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 45%|█████████████████████████████████████████████▉                                                        | 9/20 [08:59<11:16, 61.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|██████████████████████████████████████████████████▌                                                  | 10/20 [10:04<10:25, 62.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 55%|███████████████████████████████████████████████████████▌                                             | 11/20 [11:00<09:04, 60.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 1.0, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|████████████████████████████████████████████████████████████▌                                        | 12/20 [12:00<08:01, 60.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 1.0, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 65%|█████████████████████████████████████████████████████████████████▋                                   | 13/20 [12:57<06:54, 59.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70%|██████████████████████████████████████████████████████████████████████▋                              | 14/20 [13:52<05:47, 57.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   7.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   6.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 75%|███████████████████████████████████████████████████████████████████████████▊                         | 15/20 [14:52<04:53, 58.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████▊                    | 16/20 [15:52<03:56, 59.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 85%|█████████████████████████████████████████████████████████████████████████████████████▊               | 17/20 [16:45<02:51, 57.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████▉          | 18/20 [17:41<01:53, 56.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.2, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   6.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   6.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████▉     | 19/20 [18:33<00:55, 55.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [19:23<00:00, 58.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8, 'eval_metric': 'logloss', 'learning_rate': 0.02, 'max_depth': 1, 'min_child_weight': 0.5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "\t Eval position 30 done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'auc',\n",
       " 'wine_mean_metric_at_30': 0.9325163265306122,\n",
       " 'wine_time': 1184.659984588623,\n",
       " 'covertype_mean_metric_at_30': 0.7929112244897958,\n",
       " 'covertype_time': 1163.8565165996552,\n",
       " 'mean_metric_at_30': 0.862713775510204,\n",
       " 'mean_metric': 0.862713775510204}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   6.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   5.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.5; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.02, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=1, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=0.5, n_estimators=100, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.5; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, eval_metric=logloss, learning_rate=0.2, max_depth=2, min_child_weight=1.0, n_estimators=100, subsample=0.8; total time=   1.0s\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(ds, xgb_metric, 'xgb'\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , overwrite=True\n",
    "                  , save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Bayesian NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter num_samples_for_prediction for estimator BayesianNNClassifier(device=device(type='cpu'), embed=5, lr=0.001, n_layers=2,\n                     num_features=13). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbayes_net_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbayes_net\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbptt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_position_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_positions\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m result\n",
      "File \u001b[0;32m~/TransformersCanDoBayesianInference/notebooks/../tabular.py:187\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(datasets, model, method, bptt, eval_position_range, device, max_features, plot, extend_features, save, rescale_features, overwrite, max_samples, path_interfix)\u001b[0m\n\u001b[1;32m    184\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([X, torch\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], max_features \u001b[38;5;241m-\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 187\u001b[0m ds_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbptt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_position_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrescale_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_features_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ds_result):\n",
      "File \u001b[0;32m~/TransformersCanDoBayesianInference/notebooks/../tabular.py:220\u001b[0m, in \u001b[0;36mevaluate_dataset\u001b[0;34m(X, y, categorical_feats, model, bptt, eval_position_range, plot, rescale_features, max_samples)\u001b[0m\n\u001b[1;32m    218\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_position \u001b[38;5;129;01min\u001b[39;00m eval_position_range:\n\u001b[0;32m--> 220\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbptt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Eval position \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(eval_position) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m done..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/TransformersCanDoBayesianInference/notebooks/../tabular.py:304\u001b[0m, in \u001b[0;36mevaluate_position\u001b[0;34m(X, y, categorical_feats, model, bptt, eval_position, rescale_features, max_samples)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric_per_t, outputs, eval_ys[eval_position:]\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     metric_eval_pos, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_xs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_ys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_position\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric_eval_pos, outputs, eval_ys[eval_position:]\n",
      "File \u001b[0;32m~/TransformersCanDoBayesianInference/notebooks/../tabular.py:319\u001b[0m, in \u001b[0;36mbatch_pred\u001b[0;34m(metric_function, eval_xs, eval_ys, categorical_feats, start)\u001b[0m\n\u001b[1;32m    316\u001b[0m std \u001b[38;5;241m=\u001b[39m eval_x[:start]\u001b[38;5;241m.\u001b[39mstd(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m.000001\u001b[39m\n\u001b[1;32m    317\u001b[0m eval_x \u001b[38;5;241m=\u001b[39m (eval_x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n\u001b[0;32m--> 319\u001b[0m metric, output \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m metrics \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [metric]\n\u001b[1;32m    321\u001b[0m outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [output]\n",
      "File \u001b[0;32m~/TransformersCanDoBayesianInference/notebooks/../tabular.py:473\u001b[0m, in \u001b[0;36mbayes_net_metric\u001b[0;34m(x, y, test_x, test_y, cat_features)\u001b[0m\n\u001b[1;32m    471\u001b[0m clf \u001b[38;5;241m=\u001b[39m GridSearchCV(clf, param_grid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbayes\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# fit model to data\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(test_x)\n\u001b[1;32m    476\u001b[0m metric \u001b[38;5;241m=\u001b[39m metric_used(test_y\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/model_selection/_search.py:841\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    836\u001b[0m         all_candidate_params, n_splits, all_out,\n\u001b[1;32m    837\u001b[0m         all_more_results)\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 841\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    845\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1296\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/model_selection/_search.py:795\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    793\u001b[0m               n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits))\n\u001b[0;32m--> 795\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    813\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    814\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/utils/fixes.py:222\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:586\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    584\u001b[0m         cloned_parameters[k] \u001b[38;5;241m=\u001b[39m clone(v, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 586\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcloned_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    590\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[0;32m~/anaconda3/envs/GD/lib/python3.9/site-packages/sklearn/base.py:230\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    228\u001b[0m key, delim, sub_key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_params:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    231\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheck the list of available parameters \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    232\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith `estimator.get_params().keys()`.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    233\u001b[0m                      (key, \u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delim:\n\u001b[1;32m    236\u001b[0m     nested_params[key][sub_key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter num_samples_for_prediction for estimator BayesianNNClassifier(device=device(type='cpu'), embed=5, lr=0.001, n_layers=2,\n                     num_features=13). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "result = evaluate(ds, bayes_net_metric, 'bayes_net'\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , overwrite=True\n",
    "                  , save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating wine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [06:47<00:00, 20.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n",
      "Evaluating covertype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [06:23<00:00, 19.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Eval position 30 done..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'auc',\n",
       " 'wine_mean_metric_at_30': 0.9076479591836735,\n",
       " 'wine_time': 407.7067492008209,\n",
       " 'covertype_mean_metric_at_30': 0.8101428571428572,\n",
       " 'covertype_time': 383.377445936203,\n",
       " 'mean_metric_at_30': 0.8588954081632654,\n",
       " 'mean_metric': 0.8588954081632654}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(ds, catboost_metric, 'catboost'\n",
    "                  , bptt=seq_len\n",
    "                  , eval_position_range=eval_positions\n",
    "                  , device=device\n",
    "                  , max_samples=20\n",
    "                  , overwrite=True\n",
    "                  , save=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
